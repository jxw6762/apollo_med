# ApolloMed
## Usage
- [OPTIONAL] run `pip install -r requirements.txt` if you want to have this run locally. Sometimes fastparquet throws an error, but i haven't found it to be a blocker
- For part 1, the file is `run_etl.py`. To generate the outputs, run `python run_etl.py`. The results should be stored in `output_data/formatted-drug-ndc-0001-of-0001.json`
- For part 2, to generate the outputs, run `python analyze.py`. The results are stored in parquet format under `output_data/invalid_ndcs.parquet` and `output_data/valid_ndcs.parquet`
- Note: I deviated from the directions a little bit for 2.1, but please read the justification for this under the implementation section
- I uploaded the library including the output files. But feel free to delete them if you want to test from scratch. 
## Documentation:
Roughly outlining the process I would go through. This would generally include scoping, design, implementation
### Scoping:

- Problems/requirements:
    1. Download a data file from a given link, transform the package ndc key, and store it.
        - https://open.fda.gov/apis/drug/ndc/download/
        - Considerations: Tools to use? Output file format?
    2. Take a given dataset (contained in the documents folder) and generate 2 files. 
        1. First output file should be an aggregation of top `k` packages and count
        2. List of keys not found in file generated by part 1

Questions (see documents folder)

### Design
1. The idea here is to go with an 'ETL' approach.
- Extract - Download the data
  - Can debate the value of storing raw data here
- Transform - Convert package NDCs into appropriate format
- Load - Save this
    - JSON and Parquet
    
- Scheduling: Given the characteristics of this job, I would suggest going with a tool such as aws lambda or some generic tool that can run code at a regular interval. Could even be locally via cron. We could use a more sophisticated tool such as airflow if it were already spun up, however this job doesn't really take advantage of airflow's capabilities, so it feels like a sledgehammer -> crack a nut kinda scenario
- Some additional thoughts: More important than scheduling, i would probably look at ways to monitor/alert off this job (often scheduling tools also have this feature) if it is business-critical. I find that knowing when things failed/didn't run is just as important as getting to run in the first place.

2.1
1. Sanitize the data
    - expunge nulls
    - Check for valid ndc format
2. Output file with top k NDCs and # of times
    - JSON: {<ndc_key>:<count>, ...}
    - Parquet: Instead of just the top k, i'm going to include the full aggregation and store it. This is generally a more accessed approach in data

2.2
- Essentially what this is asking for is a join between the 2 datasets.
```buildoutcfg
select input.key, count(*)
from fda_dataset 
left join input
    on fda_dataset.key = input.key
where fda_dataset.key is null
group by input.key
```

###Implementation
- Part 1: Should break some of the functions into helper libraries
- Part 2:
    - for 2.1, I deivated from the directions slightly because i wanted to showcase pandas and parquet. Instead of just taking a command line input of 'k' as a parameter, i decided to output the entire dataset in parquet format. This isn't particularly useful for exercise, but if we're talking about scaling, then columnar storage formats are much better. Plus pandas is a better analytics tool (though not as good as pyspark) than just trying to analyze using vanilla python
